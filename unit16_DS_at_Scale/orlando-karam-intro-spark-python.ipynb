{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Resilient Distributed Datasets\n",
    "    - big set of objects\n",
    "    - keep each slice in many different computers\n",
    "    - immutable, not designed for read/write\n",
    "    - lazy operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Spark Variables\n",
    "\n",
    "- Broadcast variables \n",
    "    - copy is kept at each node\n",
    "- Accumulators\n",
    "    - you can only add; main node can read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional programming in python\n",
    "\n",
    "- Functional tools in python\n",
    "    - map (applies function to list, returns results to new list)\n",
    "    - filter\n",
    "    - reduce \n",
    "    - lambda\n",
    "    - itertools (chain, flatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map in Python\n",
    "\n",
    "- Apply an operation to each element of a list, return a new list with the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of how map works\n",
    "def add1(x):\n",
    "    return x+1\n",
    "\n",
    "list(map(add1, [1, 2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter in Python\n",
    "\n",
    "- select only certain elements from a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of filter\n",
    "a = [1, 2, 3, 4]\n",
    "\n",
    "def isOdd(x):\n",
    "    return x%2 == 1\n",
    "\n",
    "list(filter(isOdd, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce in Python\n",
    "\n",
    "- applies a function to all pairs of elements of a list; returns one value, not a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = range(1, 5)\n",
    "list(a)\n",
    "\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "from functools import reduce\n",
    "reduce(add, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda in Python\n",
    "\n",
    "- when doing map/reduce/filter, we end up with many tiny functions\n",
    "- lambdas allow us to define a function as a value, without giving it a name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of lambda\n",
    "x = range(0, 10)\n",
    "\n",
    "x = map(lambda x: x+1, x)\n",
    "\n",
    "list(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given\n",
    "a = [(1,2),(3,4),(5,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write an expression to get only the second elements of each tuple\n",
    "list(map((lambda t: t[1]), a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write an expression that adds up the second elements\n",
    "reduce((lambda x,y: x+y), map((lambda t: t[1]), a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write an expression that returns the odd numbers and \n",
    "reduce((lambda x,y: x+y), filter(isOdd, map((lambda t: t[0]), a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatmap\n",
    "\n",
    "- sometimes we end up with list of lists, and we want a 'flat' list\n",
    "- many functional programming languages(and Spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating RDDs in Spark\n",
    "\n",
    "- All Spark commands operate on RDDs (think big distributed list)\n",
    "- You can use sc.parallelize to go from list to RDD\n",
    "- Many commands are lazy (they don't actually compute the results until you need them)\n",
    "- In pySpark, `sc` represents your SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `sc.parallelize(range(1,10)).first()` returns 1\n",
    "\n",
    "### Simple Example(s)\n",
    "\n",
    "- `list1 = sc.parallelize(range(1, 1000))`\n",
    "- `list2 = list1.map(lambda x: x*10)` # notice lazy\n",
    "- `list2.reduce(lambda x,y: x + y)`\n",
    "- `list2.filter(lambda x: x%100 == 0).collect()`\n",
    "\n",
    "- super fast because it doesn't actually do anything unless called upon\n",
    "    - example = `.first()` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some RDD methods\n",
    "\n",
    "- Transformations\n",
    "    - `.map()`: returns a new RDD applying f to each element\n",
    "    - `.filter(f)`: returns a new RDD containing elements that satisfy f\n",
    "    - `.flatmap(f)`: returns a 'flattened' list\n",
    "- Actions\n",
    "    - `.reduce(f)`: returns a value reducing RDD elements with f\n",
    "    - `.take(n)`: returns n items from the RDD\n",
    "    - `.collect()`: returns all elements as a list\n",
    "    - `.sum()`: sum of (numeric) elements of an RDD\n",
    "        - i.e. max, min, mean..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More examples\n",
    "\n",
    "- `rdd1 = sc.parallelize(range(1,100))`\n",
    "- `rdd1.map(lambda x: x*x).sum()`\n",
    "- `rdd1.filter(lambda x: x%2==0).take(5)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "- `sc.parallelize(range(1,10)).filter(lambda x: x%3==0).reduce(lambda x,y: x*y)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading files\n",
    "\n",
    "- `sc.textFile(urlOrPath, minPartitions, useUnicode=True)`\n",
    "    - returns an RDD of strings (one per line)\n",
    "    - can read from many files, using wildcards (*)\n",
    "    - can read from hdfs\n",
    "    - normally use map right after and split/parse the lines\n",
    "- Example:\n",
    "    - `people = sc.textFile('../data/people.txt')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuples and ReduceByKey\n",
    "\n",
    "- Many times we want to group elements first, and then calculate values for each group\n",
    "- In spark, we operate on tuples <Key, Value> and we normally use reduceByKey to perform a reduce on the elements of each group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### People example/exercises\n",
    "\n",
    "- We have a `people.txt` file with following schema:\n",
    "    - Name | Gender | Age | Favorite Language\n",
    "- We can load with:\n",
    "    - `people = sc.textFile('../data/people.txt').map(lambda x: x.split('\\t\\))`\n",
    "- Find number of people by gender\n",
    "    - first get tuples like: ('M', 1), ('F',1) then reduce by key\n",
    "    - `people.map(lambda t: (t[1],1)).reduceByKey(lambda x,y: x+y).collect()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _CLICK [HERE](https://youtu.be/9xYfNznjClE?t=4503) TO CONTINUE_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
