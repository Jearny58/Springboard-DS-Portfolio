{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Resilient Distributed Datasets\n",
    "    - big set of objects\n",
    "    - keep each slice in many different computers\n",
    "    - immutable, not designed for read/write\n",
    "    - lazy operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Spark Variables\n",
    "\n",
    "- Broadcast variables \n",
    "    - copy is kept at each node\n",
    "- Accumulators\n",
    "    - you can only add; main node can read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional programming in python\n",
    "\n",
    "- Functional tools in python\n",
    "    - map (applies function to list, returns results to new list)\n",
    "    - filter\n",
    "    - reduce \n",
    "    - lambda\n",
    "    - itertools (chain, flatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map in Python\n",
    "\n",
    "- Apply an operation to each element of a list, return a new list with the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of how map works\n",
    "def add1(x):\n",
    "    return x+1\n",
    "\n",
    "list(map(add1, [1, 2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter in Python\n",
    "\n",
    "- select only certain elements from a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of filter\n",
    "a = [1, 2, 3, 4]\n",
    "\n",
    "def isOdd(x):\n",
    "    return x%2 == 1\n",
    "\n",
    "list(filter(isOdd, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce in Python\n",
    "\n",
    "- applies a function to all pairs of elements of a list; returns one value, not a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = range(1, 5)\n",
    "list(a)\n",
    "\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "from functools import reduce\n",
    "reduce(add, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda in Python\n",
    "\n",
    "- when doing map/reduce/filter, we end up with many tiny functions\n",
    "- lambdas allow us to define a function as a value, without giving it a name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of lambda\n",
    "x = range(0, 10)\n",
    "\n",
    "x = map(lambda x: x+1, x)\n",
    "\n",
    "list(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given\n",
    "a = [(1,2),(3,4),(5,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write an expression to get only the second elements of each tuple\n",
    "list(map((lambda t: t[1]), a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write an expression that adds up the second elements\n",
    "reduce((lambda x,y: x+y), map((lambda t: t[1]), a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write an expression that returns the odd numbers and \n",
    "reduce((lambda x,y: x+y), filter(isOdd, map((lambda t: t[0]), a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatmap\n",
    "\n",
    "- sometimes we end up with list of lists, and we want a 'flat' list\n",
    "- many functional programming languages(and Spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating RDDs in Spark\n",
    "\n",
    "- All Spark commands operate on RDDs (think big distributed list)\n",
    "- You can use sc.parallelize to go from list to RDD\n",
    "- Many commands are lazy (they don't actually compute the results until you need them)\n",
    "- In pySpark, `sc` represents your SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `sc.parallelize(range(1,10)).first()` returns 1\n",
    "\n",
    "### Simple Example(s)\n",
    "\n",
    "- `list1 = sc.parallelize(range(1, 1000))`\n",
    "- `list2 = list1.map(lambda x: x*10)` # notice lazy\n",
    "- `list2.reduce(lambda x,y: x + y)`\n",
    "- `list2.filter(lambda x: x%100 == 0).collect()`\n",
    "\n",
    "- super fast because it doesn't actually do anything unless called upon\n",
    "    - example = `.first()` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some RDD methods\n",
    "\n",
    "- Transformations\n",
    "    - `.map()`: returns a new RDD applying f to each element\n",
    "    - `.filter(f)`: returns a new RDD containing elements that satisfy f\n",
    "    - `.flatmap(f)`: returns a 'flattened' list\n",
    "- Actions\n",
    "    - `.reduce(f)`: returns a value reducing RDD elements with f\n",
    "    - `.take(n)`: returns n items from the RDD\n",
    "    - `.collect()`: returns all elements as a list\n",
    "    - `.sum()`: sum of (numeric) elements of an RDD\n",
    "        - i.e. max, min, mean..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More examples\n",
    "\n",
    "- `rdd1 = sc.parallelize(range(1,100))`\n",
    "- `rdd1.map(lambda x: x*x).sum()`\n",
    "- `rdd1.filter(lambda x: x%2==0).take(5)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "- `sc.parallelize(range(1,10)).filter(lambda x: x%3==0).reduce(lambda x,y: x*y)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading files\n",
    "\n",
    "- `sc.textFile(urlOrPath, minPartitions, useUnicode=True)`\n",
    "    - returns an RDD of strings (one per line)\n",
    "    - can read from many files, using wildcards (*)\n",
    "    - can read from hdfs\n",
    "    - normally use map right after and split/parse the lines\n",
    "- Example:\n",
    "    - `people = sc.textFile('../data/people.txt')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuples and ReduceByKey\n",
    "\n",
    "- Many times we want to group elements first, and then calculate values for each group\n",
    "- In spark, we operate on tuples <Key, Value> and we normally use reduceByKey to perform a reduce on the elements of each group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### People example/exercises\n",
    "\n",
    "- We have a `people.txt` file with following schema:\n",
    "    - Name | Gender | Age | Favorite Language\n",
    "- We can load with:\n",
    "    - `people = sc.textFile('../data/people.txt').map(lambda x: x.split('\\t\\))`\n",
    "- Find number of people by gender\n",
    "    - first get tuples like: ('M', 1), ('F',1) then reduce by key\n",
    "    - `people.map(lambda t: (t[1],1)).reduceByKey(lambda x,y: x+y).collect()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending programs within shell\n",
    "\n",
    "- Can use extra parameters to include python programs in your shell\n",
    "    - `--py-files` (and list of files, separated with spaces)\n",
    "        - can use `.py`, `.zip`, `.egg`\n",
    "    - `--jars` to include java jars\n",
    "    - `--packages`, `--repositories` to include maven package (java)\n",
    "    - `--files` to include arbitrary files in home folder of executor\n",
    "- Get out of pyspark\n",
    "    - Ctrl-D\n",
    "- Run it again, including `person.py` in your `--py-files`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Person with Objects\n",
    "\n",
    "- Number of people by gender\n",
    "    - `people.map(lambda t: (t.gender,1)).reduceByKey(lambda x,y: x+y).collect()\n",
    "- Let's do number of people by programming language\n",
    "- Youngest person by gender\n",
    "    - `people.map(lambda t: (t.gender, t.age)).reduceByKey(lambda x,y: min(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales example\n",
    "\n",
    "- Sales: `Day | StoreId | ProductId | QtySold\n",
    "- Load:\n",
    "    - `sales = sc.textFile('sales-data/sales_*.txt').map(lambda x: x.split('\\t'))\n",
    "- now sales is an RDD of arrays corresponding to the fields\n",
    "    - but each field is a string\n",
    "- total quantity of products sold:\n",
    "    - `sales.map(lambda x: int(x[3])).sum()`\n",
    "    \n",
    "### Example\n",
    "\n",
    "- `sales_by_store = sales.map(lambda t: (t[1], int(t[3])))`\n",
    "- `sales_by_store.reduceByKey(lambda t1, t2: t1 + t2).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins\n",
    "\n",
    "- Joins allow us to combine different RDDs\n",
    "    - each RDD is of the form <K,V> (key and value)\n",
    "    - Result is of the form <K,<V1,V2>> (notice the nesting)\n",
    "    - Joins only on equal keys (equijoin from db)\n",
    "    - Also have leftOuterJoin, rightOuterJoin and fullOuterJoin\n",
    "    - And cartesian, if you want the cartesian product, and other kinds of join, but this is potentially very slow\n",
    "    \n",
    "### Simple join example\n",
    "\n",
    "- `states_rdd = sc.parallelize(states)`\n",
    "- `populations_rdd = sc.parallelize(populations)`\n",
    "- `states_rdd.join(populations_rdd);`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New DataTable functionality\n",
    "\n",
    "- a database is like an RDD but with schema information\n",
    "    - like a table in SQL, or datatable in pandas\n",
    "    - generic objects, know their fields\n",
    "    - datatable knows all its columns\n",
    "    - all 'rows' are of the same kind (but there are nulls, and arrays, etc)\n",
    "- we need to either read from places with schemas, or add schema info\n",
    "- we specify queries on them (similar to RDD, or through SQL), but there's a query optimizer\n",
    "    - slightly harder to do general aggregates\n",
    "- much smaller python tax!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datatable\n",
    "\n",
    "- `.select` - like map, can use strings or columns\n",
    "    - `people.select('name', people.age+1).show()`\n",
    "- `.filter` - filter certain rows\n",
    "    - `people.filter(people.age>30)\n",
    "- `.show` - display nicely\n",
    "- Pandas syntax for filter\n",
    "    - `people[people.gender=='F']\n",
    "- GroupBy returns a grouped RDD\n",
    "    - `people.groupBy(people.gender).count()`\n",
    "- Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Considerations\n",
    "\n",
    "- Spark in python is slower than in scala due to translation\n",
    "    - Spark processes are running in JVM\n",
    "    - Need to send objects back and forth between JVM and python\n",
    "- Datatable avoids this translation, it all lives in JVM\n",
    "    - until last step to client\n",
    "- Datatable can optimize better\n",
    "    - but you lose some control\n",
    "- Shuffling (join/reduce) is more expensive\n",
    "    - partitioning can help some"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Performance\n",
    "\n",
    "- RDD is:\n",
    "    - Lineage\n",
    "        - set of pertitions/splits\n",
    "        - list of dependencies on parent RDDs\n",
    "        - function to comute each partition given its parents\n",
    "    - Optimized Execution\n",
    "        - partitioner - which objects go on which partitions\n",
    "            - partitioning can help when shuffling\n",
    "        - preferred location for each partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _CLICK [HERE](https://youtu.be/9xYfNznjClE) FOR VIDEO_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
