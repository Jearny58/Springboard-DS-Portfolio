{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "- contains 224,316 chest radiographs of 65,240 patients\n",
    "- designed a laberl to automatically detect the presence of 14 observations in radiology reports\n",
    "    - investigate different approaches to using uncertainty labels that output probability of these observations given available frontal and lateral radiographs\n",
    "- Validation set = 200 chest radiographic studies\n",
    "    - manually annotated by 3 board-certified radiologists\n",
    "        - found different uncertainty approachs were useful for different pathologies\n",
    "- Results, model ROC and PR curves lie above all 3 radiologist operating points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "- Automated chest radiograph interpretation at level of practicing radiologists\n",
    "    - benefit in many medical settings\n",
    "        - improved workflow prioritization and clinical decision support\n",
    "        - large-scale screening and global population health initiatives\n",
    "- Designed labeler that extracted observations from free-text radiology reports\n",
    "    - captured uncertainties present in reports by using an uncertainty label\n",
    "- Pay particular attention to uncertainty labels\n",
    "\n",
    "### Table 1 From CheXpert Paper\n",
    "![Table 1 From Chexpert Paper](images/chexpert_table_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### _Data Collection and Label Selection_\n",
    "\n",
    "- collected chest radiographic studies from Stanford Hospital, performed between October 2002 and July 2017\n",
    "    - from both inpatient and outpatient centers, along with associated radiology reports\n",
    "    - from these sampled 1000 reports for manual review by board-certified radiologist\n",
    "- determined 14 observations (i.e. pathologies) based on prevalence in reports and clinical relevance\n",
    "    - `Pneumonia` was included as a label to represent images that suggest primary infection as diagnosis\n",
    "    - `No Finding` observation captured absence of all pathologies\n",
    "    \n",
    "### _Label Extraction from Radiology Reports_\n",
    "\n",
    "- team developed automated rule-based labeler to extract observations from free text radiology reports\n",
    "    - set up in three distinct stages:\n",
    "        - __mention extraction__\n",
    "        - __mention classification__\n",
    "        - __mention aggregation__\n",
    "        \n",
    "#### Mention Extraction\n",
    "\n",
    "- extracts mentions from list of observation from _Impression_ section of report\n",
    "    - summarizes key findings in study\n",
    "    - team also put together manually curated list of phrases to match alternative names for pathologies in reports\n",
    "    \n",
    "#### Mention Classification\n",
    "\n",
    "- after extraction, aim is to classify them as negative, uncertain or positive\n",
    "- `uncertain` label can catch both uncertainty of radiologist in diagnosis as well as ambiguity inherent in report (__HOW?__)\n",
    "- Is 3-phase pipeline consisting of:\n",
    "    - pre-negation uncertainty\n",
    "    - negation\n",
    "    - post-negation uncertainty\n",
    "        - if match is found, mention is classified accordingly \n",
    "        - if mention is not matched in any of the phases, it is classified as positive\n",
    "- Rules for mention classification designed on universal dependency parse of report\n",
    "    - first, split and tokenize sentences using `NLTK`\n",
    "    - then, sentences parsed using Bllip parser trained using __David McClosky's__ biomedical model [see here](https://nlp.stanford.edu/~mcclosky/papers/dmcc-thesis-2010.pdf)\n",
    "    - finally, universal dependency graph of each sentence is computed using Stanford CoreNLP [see here](https://nlp.stanford.edu/pubs/USD_LREC14_paper_camera_ready.pdf)\n",
    "    \n",
    "#### Mention Aggregation\n",
    "\n",
    "- use classification for each mention of observations to determine label from 12 pathologies as well as `Support Devices` and `No Finding`\n",
    "    - observations with at least one mention is assigned a positive (1) label\n",
    "    - observation assigned uncertain (u) label if no positively classified mentions and at least one uncertain mention\n",
    "    - observation assigned negative label if there is at least one negatively classified mention\n",
    "    - assign _blank_ if there is no mention of an observation\n",
    "    - `No Finding` observation assigned a positive label (1) if no pathology classified as positive or uncertain\n",
    "    \n",
    "# Table 2 From Chexpert Paper\n",
    "![Table 2](images/chexpert_table_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeler Results\n",
    "\n",
    "### Report Evaluation Set\n",
    "- report evaluation set = 1000 radiology reports from 1000 distinct randomly sampled patients\n",
    "    - do not overlap with patients whose studies were used to develop the labeler\n",
    "- two board-certified radiologists (w/o access to additional info) label each observation\n",
    "    - confidently present (1)\n",
    "    - confidently absent (0)\n",
    "    - uncertainly present (u)\n",
    "    - not mentioned (blank)\n",
    "- resulting annotation serve as ground truth on the report evaluation set\n",
    "\n",
    "### Comparison to NIH labeler\n",
    "- compared labeler against method used in NIH medical image dataset\n",
    "- Table 2 (see above) shows the performace of the CheXpert labeler vs. NIH labeler\n",
    "    - across all observations CheXpert labeler achieved higher F1 score\n",
    "    - The F1 score: weighted average of the precision and recall, with the best value at 1 and worst score at 0. \n",
    "        - The relative contribution of precision and recall to the F1 score are equal. \n",
    "        - The formula for the F1 score is:\n",
    "`F1 = 2 * (precision * recall) / (precision + recall)`\n",
    "-  Three key differences between CheXpert method and NIH method\n",
    "    - did __not__ use automatic mention extractors like MetaMap or DNorm\n",
    "    - incorporated several additional rules to capture large variation in ways negation and uncertainty are conveyed\n",
    "    - split uncertainty classification of mentions into pre-negation and post-negation\n",
    "        - allowed them to resolve cases of uncertainty rules double matching with negation rules in the reports\n",
    "        - Example, the following phrase `cannot exclude pneumothorax` conveys uncertainty in the presence of pneumothorax\n",
    "        - without pre-negation stage, 'pneumothorax match is classified as negative due to 'exclude XXX' rule\n",
    "        - by applying 'cannot exclude' rule in pre-negation, this observation can be correctly classified as uncertain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trained models that take as input a single-view chest radiograph and output the probability of each of the 14 observations\n",
    "    - when more than one view is available, the models output the maximum probability of the observations across the views\n",
    "    \n",
    "### Uncertainty Approaches\n",
    "- training labels in the dataset for each observation are 0 (negative), 1 (positive), or _u_ (uncertain)\n",
    "\n",
    "#### Ignoring (_U-Ignore_)\n",
    "- simple apprach is to ignore the _u_ labels during training\n",
    "    - can serve as baseline to compare approaches which incoroporate uncertainty labels\n",
    "- optimized the sum of the _masked_ binary cross-entropy losses over observations\n",
    "    - masked the loss for the observations which are marked as uncertain for the study\n",
    "- Can produce biased models if the cases are not missing completely at random\n",
    "- In this dataset, uncertainty labels are quite prevalent for some observations\n",
    "    - Consolidation, for example has uncertainty label ~2x as prevalent as positive label\n",
    "    - as a result, this approach ignores a large proportion of labels (i.e. reduces effective size of the dataset)\n",
    "    \n",
    "#### Binary Mapping\n",
    "- investigated whether the uncertain labels for any of the observations could be replaced by 0 or 1 label\n",
    "    - map all instances of _u_ to 0 (_U-Zeroes_ model), or all to 1 (_U-Ones_ model)\n",
    "- similar to zero imputation strategies in stats\n",
    "- however if uncertainty label does convey useful info to classifier, then it can distort decision making and degrade performance\n",
    "\n",
    "#### Self-Training\n",
    "- another framework is to consider uncertainty labels as unlabeled examples\n",
    "    - lending its way to semi-supervised learning\n",
    "    - _multi-label learning with missing labels_ (MLML)\n",
    "        - aims to handle multi-label classification given training instances that have a partial annotation of their labels\n",
    "- Investigated self-training approach (_U-SelfTrained_) for using the uncertainty label\n",
    "    - first trained a model using _U-Ignore_ (ignores _u_ labels during training) to convergence\n",
    "    - then used the model to make predictions that re-label each of the uncertainty labels with the probability prediction outputted by the model\n",
    "    - do __not__ replace any instances of 1 or 0s\n",
    "    - on the relabeled examples, set up loss as the mean of the binary cross-entropy losses over the observations\n",
    "    \n",
    "#### 3-Class Classification (_U-MultiClass_)\n",
    "- this approach treats _u_ label as its own class\n",
    "    - as opposed to mapping it to a binary label for each of the 14 observations\n",
    "- hypothesis: can better incorporate information from image by supervising uncertainty\n",
    "    - allows netowrk to find own representation of uncertainty on different pathologies\n",
    "- Output the probability of each of the 3 possible classes (equaling 1)\n",
    "    - set up loss as the mean of the multi-class cross-entropy losses over the observations\n",
    "    - at test time, output probability of positive label after applying softmax restriced to the positive and negative classes\n",
    "    \n",
    "### Training Procedure\n",
    "- Followed same architecture and training process for each of the uncertainty approaches\n",
    "- Experimented with the following:\n",
    "    - `ResNet152`\n",
    "    - `DenseNet121` (__found to have the best results__)\n",
    "    - `Inception-v4`\n",
    "    - `SE-ResNeXt101`\n",
    "- Images fed into network with:\n",
    "    - size = 320 x 320 pixels\n",
    "    - used Adam optimzer with default β-parameters of β1 = 0.9, β2 = 0.999\n",
    "    - learning rate = 1 x 10e-4\n",
    "        - was fixed for the duration of the training\n",
    "    - batches are sampled using a fixed batch size of 16 images\n",
    "    - trained for 3 epochs, saving checkpoints every 4800 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Results\n",
    "\n",
    "### Figure 3 from CheXpert Paper\n",
    "![Figure 3](images/figure_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set\n",
    "- 200 studies from 200 patients\n",
    "    - randomly sampled from full dataset\n",
    "    - Three board-certified radiologists annotated each of the studies in validation set\n",
    "        - classified each observation into present, uncertain likely, uncertain unlikely and absent\n",
    "        - their annotations were binarized such that all present and uncertain likely cases treated as positive & all absent and uncertain unlikely cases treated as negative\n",
    "        \n",
    "### Comparison of Uncertainty Approaches\n",
    "\n",
    "#### Procedure\n",
    "- evaluate approaches using area under the reciever operating characteristic curve (AUC) metric\n",
    "- focus on the evaluation of 5 observations which we call __competition tasks__\n",
    "    - based on clinical importance and prevalence in validation set:\n",
    "        - Atelectasis\n",
    "        - Cardiomegaly\n",
    "        - Consolidation\n",
    "        - Edema\n",
    "        - Pleural Effusion\n",
    "        \n",
    "#### Model Selection\n",
    "- for each of uncertainty approaches, chose best 10 checkpoints per run using avg. AUC across competition tasks\n",
    "    - run each model 3x, take ensemble of 30 generated checkpoints on validation set\n",
    "        - computed the mean of the output probabilities over the 30 models\n",
    "        \n",
    "#### Results\n",
    "- On Atelectasis, _U-Ones model_ (AUC=0.858) significantly outperformed _U-Zeroes model_ (AUC=0.811)\n",
    "- On Cardiomegaly, _U-MultiClass model_ (AUC=0.854) performed significantly better than _U-Ignore model_ (AUC=0.828)\n",
    "- For Consolidation, Edema and Pleural Effusion, did not find the best models to be significantly better than the worst\n",
    "\n",
    "#### Analysis\n",
    "- Ignoring uncertainty label is not effective approach to handling uncertainty\n",
    "    - particularly ineffective on Cardiomegaly\n",
    "        - most of uncertain Cardiomegaly cases are borderline (i.e. \"minimal cardiac enlargement\")\n",
    "        - if ignored can cause model to perform poorly on cases that are hard to distinguish\n",
    "    - _U-MultiClass_ approach could enable model to better disambiguate boderline cases\n",
    "- Detection of Atelectasis & Edema, _U-Ones_ approach has high performance\n",
    "    - hints that uncertainty label for this observation effectively utilized when treated as positive\n",
    "- For Consolidation lebel, _U-Zeros_ approach performed the best\n",
    "    - noted that Atelectasis and Consolidation often mentioned together in radiology reports\n",
    "    - example: 'findings may represent atelectasis vs. consolidation' is very common\n",
    "        - for this, the labeler assigned uncertain for both observations\n",
    "        - found from ground truth panel, many of these cases resolved as Atelectasis-positive and Consolidation-negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Results\n",
    "\n",
    "- selected final model based on best performing ensemble on each competition task on the validation set\n",
    "    - _U-Ones_ for Atelectasis and Edema\n",
    "    - _U-MultiClass_ for Cardiomegaly and Pleural Effusion\n",
    "    - _U-SelfTrained_ for Consolidation\n",
    "    \n",
    "### Test Set\n",
    "- consisted of 500 studies from 500 patients randomly sampled from 1000 studies in report test set\n",
    "- individually annotated by eight board-certified radiologists\n",
    "    - majority vote of 5 radiologist annotations serves as strong ground truth\n",
    "    \n",
    "### Comparison to Radiologists\n",
    "\n",
    "#### Procedure\n",
    "- computed sensitivity (recall), specificity, and precision against test set ground truth\n",
    "- to compare model to radiologists, plotted radiologist operating points with model on both the ROC and Precision-Recall (PR) space\n",
    "    - examined whether points lie below curves to determine if model is superior\n",
    "    \n",
    "#### Results\n",
    "- best AUC on Pleural Effusion (0.97) and worst on Atelectasis (0.85)\n",
    "    - all other AUC's were > 0.9\n",
    "- On Cardiomegaly, Edema, and PLeural Effusion, model achieves higher performance than all 3 radiologists but not their majority vote\n",
    "- On Consolidation, model performance exceeds 2 of the 3 radiologists\n",
    "- Atelectasis, all 3 radiologists perform better than the model\n",
    "\n",
    "#### Limitations\n",
    "- First, neither the radiologists nor the model had access to patient history or previous exmainations\n",
    "    - has been shown to decrease diagnostic performance in chest radiograph interpretation\n",
    "- Second, no statistical test was performed to assess whether difference between the performance of the model and radiologists was statistically significant\n",
    "\n",
    "### Visualization\n",
    "- visualized areas of radiograph which model predicts to be most indicative of each observation\n",
    "- uses Gradient-weighted Class Activation Mappings (Grad-CAMs)\n",
    "    - utilize gradient of an output class into the final convolutional layer to produce low resolution map highlighting portions of image important to detection of output class\n",
    "- Constructed the map using the gradient of the final linear layer as the weights \n",
    "    - then performed a weighted sum of the final feature maps using those weights\n",
    "    - upscaled resulting map to the dimensions of the original image\n",
    "    - overlayed map on the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existing Chest Radiograph Datasets\n",
    "\n",
    "- Indiana Network for Patient Care hosts the OpenI dataset\n",
    "    - consists of 7,470 front-view radiographs and radiology reports that have been alebeled with key findings by human annotators\n",
    "- National Cancer Institute hosts the PLCO Lung dataset\n",
    "    - contains ~185k full resolution images\n",
    "    - due to nature of collection process though, has low prevalence of clinically important pathologies\n",
    "        - such as Pneumothorax, Consolidation, Effusion, and Cardiomegaly\n",
    "- MIMIC-CXR dataset from MIT (recently announced)\n",
    "- ChextX-ray14 dataset from NIH\n",
    "    - using this as benchmark is problematic, labels in test set extracted from reports using an automatic labeler"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
