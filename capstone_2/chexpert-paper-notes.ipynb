{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "- contains 224,316 chest radiographs of 65,240 patients\n",
    "- designed a laberl to automatically detect the presence of 14 observations in radiology reports\n",
    "    - investigate different approaches to using uncertainty labels that output probability of these observations given available frontal and lateral radiographs\n",
    "- Validation set = 200 chest radiographic studies\n",
    "    - manually annotated by 3 board-certified radiologists\n",
    "        - found different uncertainty approachs were useful for different pathologies\n",
    "- Results, model ROC and PR curves lie above all 3 radiologist operating points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "- Automated chest radiograph interpretation at level of practicing radiologists\n",
    "    - benefit in many medical settings\n",
    "        - improved workflow prioritization and clinical decision support\n",
    "        - large-scale screening and global population health initiatives\n",
    "- Designed labeler that extracted observations from free-text radiology reports\n",
    "    - captured uncertainties present in reports by using an uncertainty label\n",
    "- Pay particular attention to uncertainty labels\n",
    "\n",
    "### Table 1 From CheXpert Paper\n",
    "![Table 1 From Chexpert Paper](images/chexpert_table_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### _Data Collection and Label Selection_\n",
    "\n",
    "- collected chest radiographic studies from Stanford Hospital, performed between October 2002 and July 2017\n",
    "    - from both inpatient and outpatient centers, along with associated radiology reports\n",
    "    - from these sampled 1000 reports for manual review by board-certified radiologist\n",
    "- determined 14 observations (i.e. pathologies) based on prevalence in reports and clinical relevance\n",
    "    - `Pneumonia` was included as a label to represent images that suggest primary infection as diagnosis\n",
    "    - `No Finding` observation captured absence of all pathologies\n",
    "    \n",
    "### _Label Extraction from Radiology Reports_\n",
    "\n",
    "- team developed automated rule-based labeler to extract observations from free text radiology reports\n",
    "    - set up in three distinct stages:\n",
    "        - __mention extraction__\n",
    "        - __mention classification__\n",
    "        - __mention aggregation__\n",
    "        \n",
    "#### Mention Extraction\n",
    "\n",
    "- extracts mentions from list of observation from _Impression_ section of report\n",
    "    - summarizes key findings in study\n",
    "    - team also put together manually curated list of phrases to match alternative names for pathologies in reports\n",
    "    \n",
    "#### Mention Classification\n",
    "\n",
    "- after extraction, aim is to classify them as negative, uncertain or positive\n",
    "- `uncertain` label can catch both uncertainty of radiologist in diagnosis as well as ambiguity inherent in report (__HOW?__)\n",
    "- Is 3-phase pipeline consisting of:\n",
    "    - pre-negation uncertainty\n",
    "    - negation\n",
    "    - post-negation uncertainty\n",
    "        - if match is found, mention is classified accordingly \n",
    "        - if mention is not matched in any of the phases, it is classified as positive\n",
    "- Rules for mention classification designed on universal dependency parse of report\n",
    "    - first, split and tokenize sentences using `NLTK`\n",
    "    - then, sentences parsed using Bllip parser trained using __David McClosky's__ biomedical model [see here](https://nlp.stanford.edu/~mcclosky/papers/dmcc-thesis-2010.pdf)\n",
    "    - finally, universal dependency graph of each sentence is computed using Stanford CoreNLP [see here](https://nlp.stanford.edu/pubs/USD_LREC14_paper_camera_ready.pdf)\n",
    "    \n",
    "#### Mention Aggregation\n",
    "\n",
    "- use classification for each mention of observations to determine label from 12 pathologies as well as `Support Devices` and `No Finding`\n",
    "    - observations with at least one mention is assigned a positive (1) label\n",
    "    - observation assigned uncertain (u) label if no positively classified mentions and at least one uncertain mention\n",
    "    - observation assigned negative label if there is at least one negatively classified mention\n",
    "    - assign _blank_ if there is no mention of an observation\n",
    "    - `No Finding` observation assigned a positive label (1) if no pathology classified as positive or uncertain\n",
    "    \n",
    "# Table 2 From Chexpert Paper\n",
    "![Table 2](images/chexpert_table_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeler Results\n",
    "\n",
    "### Report Evaluation Set\n",
    "- report evaluation set = 1000 radiology reports from 1000 distinct randomly sampled patients\n",
    "    - do not overlap with patients whose studies were used to develop the labeler\n",
    "- two board-certified radiologists (w/o access to additional info) label each observation\n",
    "    - confidently present (1)\n",
    "    - confidently absent (0)\n",
    "    - uncertainly present (u)\n",
    "    - not mentioned (blank)\n",
    "- resulting annotation serve as ground truth on the report evaluation set\n",
    "\n",
    "### Comparison to NIH labeler\n",
    "- compared labeler against method used in NIH medical image dataset\n",
    "- Table 2 (see above) shows the performace of the CheXpert labeler vs. NIH labeler\n",
    "    - across all observations CheXpert labeler achieved higher F1 score\n",
    "    - The F1 score: weighted average of the precision and recall, with the best value at 1 and worst score at 0. \n",
    "        - The relative contribution of precision and recall to the F1 score are equal. \n",
    "        - The formula for the F1 score is:\n",
    "`F1 = 2 * (precision * recall) / (precision + recall)`\n",
    "-  Three key differences between CheXpert method and NIH method\n",
    "    - did __not__ use automatic mention extractors like MetaMap or DNorm\n",
    "    - incorporated several additional rules to capture large variation in ways negation and uncertainty are conveyed\n",
    "    - split uncertainty classification of mentions into pre-negation and post-negation\n",
    "        - allowed them to resolve cases of uncertainty rules double matching with negation rules in the reports\n",
    "        - Example, the following phrase `cannot exclude pneumothorax` conveys uncertainty in the presence of pneumothorax\n",
    "        - without pre-negation stage, 'pneumothorax match is classified as negative due to 'exclude XXX' rule\n",
    "        - by applying 'cannot exclude' rule in pre-negation, this observation can be correctly classified as uncertain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
